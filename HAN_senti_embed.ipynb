{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'   #指定第一块GPU可用\n",
    "gpu_num = 1\n",
    "from keras.utils import multi_gpu_model\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.8  # 程序最多只能占用指定gpu50%的显存\n",
    "config.gpu_options.allow_growth = True      #程序按需申请内存\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "# !pip install paramiko\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda, Masking\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "#from util.util_functions import getWordIdx\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train_copus_padded data from .pickle file\n",
    "file = open('pickle_data/train_copus_pad.pickle','rb')\n",
    "train_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/train_copus_pos_pad.pickle','rb')\n",
    "train_copus_pos_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/train_copus_neg_pad.pickle','rb')\n",
    "train_copus_neg_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_copus_pad.pickle','rb')\n",
    "test_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_copus_pos_pad.pickle','rb')\n",
    "test_copus_pos_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_copus_neg_pad.pickle','rb')\n",
    "test_copus_neg_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','rb')\n",
    "vocab_to_int_train = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/embedding_matrix','rb')\n",
    "embedding_matrix = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/train_label.pickle','rb')\n",
    "train_label = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_label.pickle','rb')\n",
    "test_label = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "# file = open('pickle_data/sentiword_matrix.pickle','rb')\n",
    "# sentiword_matrix = pickle.load(file)\n",
    "# file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 105616\n",
      "max sent number in a review: 30 \n",
      "max words in a sentence: 235\n"
     ]
    }
   ],
   "source": [
    "# print('train test data shape:',train_copus_padded.shape, test_copus_padded.shape)\n",
    "# print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "# print('snetiword_matrix shape:', sentiword_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int_train)\n",
    "print('vocabulary size:', vocab_size)\n",
    "# the maximal length of every sentence\n",
    "MAX_SENTS = train_copus_padded.shape[1]\n",
    "MAX_SENT_LENGTH = train_copus_padded.shape[2]\n",
    "print('max sent number in a review:', MAX_SENTS, '\\nmax words in a sentence:', MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention layer\n",
    "\n",
    "class AttLayer_joint(Layer):\n",
    "    def __init__(self, attention_dim, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer_joint, self).__init__( **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #assert len(input_shape) == 3\n",
    "        assert isinstance(input_shape, list)\n",
    "        self.W = K.variable(self.init((input_shape[0][-1], self.attention_dim)))\n",
    "        self.W2 = K.variable(self.init((input_shape[1][-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.W2,self.b, self.u]\n",
    "        super(AttLayer_joint, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=[None,None]):\n",
    "        # current mask shape: [batch_size, set_len]\n",
    "        mask = None\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=[None,None]):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        p = K.dot(x[0], self.W)+K.dot(x[1], self.W2)\n",
    "        uit = K.tanh(K.bias_add(p, self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask[0] is not None:\n",
    "#             Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask[0], K.floatx())\n",
    "            \n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        \n",
    "        weighted_input = x[0] * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        return (input_shape[0][0], input_shape[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # current shape: [batch_size, set_len]\n",
    "        mask = None\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):       \n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "gru_dim = 100\n",
    "dropout_rate = 0.3\n",
    "atten_dim = 100\n",
    "dense_dim = 50\n",
    "\n",
    "batch_size = 50\n",
    "epoch_num = 10\n",
    "\n",
    "senti_embed_dim = 40 #try 20 to 50\n",
    "\n",
    "categorical_label = True\n",
    "\n",
    "if categorical_label:\n",
    "    train_label_cat = np_utils.to_categorical(train_label)\n",
    "#     test_label_cat = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer1 = Embedding(vocab_size, embedding_matrix.shape[1], input_length=MAX_SENT_LENGTH, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "#embedding_layer2 = Embedding(vocab_size,sentiword_matrix.shape[1],input_length=MAX_SENT_LENGTH,\n",
    "                            # weights=[sentiword_matrix], trainable=False)\n",
    "embedding_layer_pos = Embedding(8, senti_embed_dim ,input_length=MAX_SENT_LENGTH,\n",
    "                              trainable=True)\n",
    "embedding_layer_neg = Embedding(8, senti_embed_dim ,input_length=MAX_SENT_LENGTH,\n",
    "                              trainable=True)\n",
    "# embedding_layer_final = keras.layers.concatenate([embedding_layer_pos, embedding_layer_neg])sent_embedding\n",
    "#吧pos和neg concatenate到一起\n",
    "#sentiword_matrix.shape[1]\n",
    "\n",
    "rnn_layer1 = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "rnn_layer2 = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "rnn_layer3 = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "rnn_layer4 = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "# rnn_layer = GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)\n",
    "\n",
    "\n",
    "# cnn_layer = Convolution1D(nb_filter=50,filter_length=3,border_mode='same',activation='tanh',subsample_length=1)\n",
    "# max_pooling_layer = GlobalMaxPooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tzjsl\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "tracking <tf.Variable 'att_layer_1/Variable:0' shape=(200, 100) dtype=float32> W\n",
      "tracking <tf.Variable 'att_layer_1/Variable_1:0' shape=(100,) dtype=float32> b\n",
      "tracking <tf.Variable 'att_layer_1/Variable_2:0' shape=(100, 1) dtype=float32> u\n",
      "tracking <tf.Variable 'att_layer_2/Variable:0' shape=(200, 100) dtype=float32> W\n",
      "tracking <tf.Variable 'att_layer_2/Variable_1:0' shape=(100,) dtype=float32> b\n",
      "tracking <tf.Variable 'att_layer_2/Variable_2:0' shape=(100, 1) dtype=float32> u\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 235)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 235, 300)          31684800  \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 235, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 235, 200)          240600    \n",
      "_________________________________________________________________\n",
      "att_layer_2 (AttLayer)       (None, 200)               20200     \n",
      "=================================================================\n",
      "Total params: 31,945,600\n",
      "Trainable params: 260,800\n",
      "Non-trainable params: 31,684,800\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 235)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 235)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 235, 20)      160         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 235, 20)      160         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 235, 40)      0           embedding_2[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 235, 40)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 235, 200)     84600       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_1 (AttLayer)          (None, 200)          20200       bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 105,120\n",
      "Trainable params: 105,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "sentence_pos_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "sentence_neg_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer1(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "sentiword_embedding_pos = embedding_layer_pos(sentence_pos_input)\n",
    "sentiword_embedding_neg = embedding_layer_neg(sentence_neg_input)\n",
    "\n",
    "sentiword_embedding = keras.layers.concatenate([sentiword_embedding_pos, sentiword_embedding_neg])\n",
    "\n",
    "# mask out padding tokens\n",
    "sent_embedding = Masking(mask_value=0., input_shape=(MAX_SENT_LENGTH, embedding_matrix.shape[1]))(sent_embedding)\n",
    "sentiword_embedding = Masking(mask_value=0., input_shape=(MAX_SENT_LENGTH, embedding_matrix.shape[1]))(sentiword_embedding)\n",
    "\n",
    "sent_rnn = rnn_layer1(sent_embedding)#sent_lstm\n",
    "sentiword_rnn=rnn_layer2(sentiword_embedding)\n",
    "\n",
    "#sent_att = AttLayer_joint(atten_dim)([sent_rnn,sentiword_rnn])#atten_dim\n",
    "\n",
    "sentiword_att = AttLayer(atten_dim)(sentiword_rnn)\n",
    "sent_att = AttLayer(atten_dim)(sent_rnn)\n",
    "\n",
    "\n",
    "sentEncoder = Model(sentence_input,sent_att)\n",
    "sentiwordEncoder = Model([sentence_pos_input,sentence_neg_input],sentiword_att)\n",
    "\n",
    "\n",
    "sentEncoder.summary()\n",
    "sentiwordEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-3f2dd3ec3e7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# reviewsenti_encoder = TimeDistributed(Lambda(lambda x: sentiwordEncoder([x[:,:MAX_SENT_LENGTH], x[:,MAX_SENT_LENGTH:]])))(mixedInput)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msingle_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_SENT_LENGTH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mMAX_SENT_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mreview_pos_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mMAX_SENT_LENGTH\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mreview_neg_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMAX_SENT_LENGTH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[0;32m    170\u001b[0m                                    'dimension.')\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# build document encoder model\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_pos_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_neg_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)   # out shape: (None, MAX_SENTS, hiddendim)\n",
    "mixedInput = keras.layers.concatenate([review_pos_input, review_neg_input])\n",
    "reviewsenti_encoder = TimeDistributed(Lambda(lambda x: sentiwordEncoder([x[:,:MAX_SENT_LENGTH], x[:,MAX_SENT_LENGTH:]])))(mixedInput)\n",
    "\n",
    "# single_input = keras.Input((MAX_SENT_LENGTH + MAX_SENT_LENGTH),)\n",
    "# review_pos_input = Lambda(lambda x: x[ :,:MAX_SENT_LENGTH])(single_input)\n",
    "# review_neg_input = Lambda(lambda x: x[ :,MAX_SENT_LENGTH:])(single_input)\n",
    "# review_encoder = TimeDistributed(sentEncoder)(single_input)                 \n",
    "\n",
    "# mask out padding sentences\n",
    "review_encoder = Masking(mask_value=0., input_shape=(MAX_SENTS, gru_dim*2))(review_encoder)\n",
    "reviewsenti_encoder = Masking(mask_value=0., input_shape=(MAX_SENTS, gru_dim*2))(reviewsenti_encoder)\n",
    "\n",
    "rnn_out1 = rnn_layer3(review_encoder) # (batch_size, timesteps, gru_dimx2)\n",
    "rnn_out2 = rnn_layer4(reviewsenti_encoder)\n",
    "\n",
    "att_out = AttLayer(atten_dim)(rnn_out1)\n",
    "att_s_out= AttLayer(atten_dim)(rnn_out2)\n",
    "att_out = Dropout(dropout_rate)(att_out)\n",
    "att_s_out = Dropout(dropout_rate)(att_s_out)\n",
    "\n",
    "main_out= keras.layers.concatenate([att_out,att_s_out])\n",
    "\n",
    "dense = Dense(dense_dim, activation='tanh')(main_out) #try \"relu\" as activation function\n",
    "dense = Dropout(dropout_rate)(dense)\n",
    "\n",
    "if categorical_label:\n",
    "    preds = Dense(2, activation='softmax')(dense) # categorical output\n",
    "    model = Model([review_input, review_pos_input, review_neg_input], preds) #rewiew_input换成一个list里面包含pos和neg input：[review_input, review_pos_input, review_neg_input]\n",
    "    #print(model.summary())\n",
    "    # Replicates `model` on multiple GPUs.\n",
    "    # This assumes that your machine has 'gpus' available GPUs.\n",
    "    if gpu_num>1:\n",
    "        model = multi_gpu_model(model, gpus=gpu_num)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "# else:\n",
    "#     preds = Dense(1, activation='sigmoid')(dense)\n",
    "#     model = Model(review_input, preds)\n",
    "#     print(model.summary())\n",
    "#     # Replicates `model` on multiple GPUs.\n",
    "#     # This assumes that your machine has 'gpus' available GPUs.\n",
    "#     if gpu_num>1:\n",
    "#         model = multi_gpu_model(model, gpus=gpu_num)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      "11450/25000 [============>.................] - ETA: 3:46 - loss: 0.6757 - acc: 0.5781"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-110f381c4d7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training for epoch {}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#[train_copus_padded,train_copus_pos_padded,train_copus_neg_padded]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcategorical_label\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_copus_padded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_copus_pos_padded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_copus_neg_padded\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtrain_label_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#train_copus_paded->list [train_copus_paded, pos_padded, neg_padded]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_copus_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))#[train_copus_padded,train_copus_pos_padded,train_copus_neg_padded]\n",
    "    if categorical_label:\n",
    "        model.fit([test_copus_padded,test_copus_pos_padded,test_copus_neg_padded] , train_label_cat, batch_size=batch_size,epochs=1) #train_copus_paded->list [train_copus_paded, pos_padded, neg_padded]\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict([test_copus_padded,test_copus_pos_padded,test_copus_neg_padded], batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=5, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
