{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admusr/anaconda2/envs/python3_pengfei/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"         # 3 is can change to 0-3\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util.util_functions import getWordIdx\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train_copus_padded data from .pickle file\n",
    "file = open('pickle_data/train_copus_pad.pickle','rb')\n",
    "train_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_copus_pad.pickle','rb')\n",
    "test_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','rb')\n",
    "vocab_to_int_train = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/embedding_matrix','rb')\n",
    "embedding_matrix = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/train_label.pickle','rb')\n",
    "train_label = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_label.pickle','rb')\n",
    "test_label = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test data shape: (25000, 36, 224) (25000, 36, 224)\n",
      "embedding_matrix shape: (97162, 300)\n",
      "vocabulary size: 97162\n",
      "max sent number in a review: 36 \n",
      "max words in a sentence: 224\n"
     ]
    }
   ],
   "source": [
    "print('train test data shape:',train_copus_padded.shape, test_copus_padded.shape)\n",
    "print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int_train)\n",
    "print('vocabulary size:', vocab_size)\n",
    "# the maximal length of every sentence\n",
    "MAX_SENTS = train_copus_padded.shape[1]\n",
    "MAX_SENT_LENGTH = train_copus_padded.shape[2]\n",
    "print('max sent number in a review:', MAX_SENTS, '\\nmax words in a sentence:', MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment word filter construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load senti_lexicon extracted from SentiWordNet\n",
    "file = open('pickle_data/senti_lexicon.pickle','rb')\n",
    "senti_lexicon = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the sentiment words to integer based on vocab2int\n",
    "senti2int = [getWordIdx(word, vocab_to_int_train) for word in senti_lexicon if getWordIdx(word, vocab_to_int_train)!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the filter weights based on the sentiment words&vocab2int&embedding_matrix\n",
    "def Find_Filter_Weight(senti2int):\n",
    "    \"\"\"sentiwords is the list\"\"\"\n",
    "    word_filter_weights = []\n",
    "    bias_weights = []\n",
    "    filter_len = 1\n",
    "    for i in senti2int:\n",
    "        vector = embedding_matrix[i]  # shape: 300\n",
    "        vector = np.expand_dims(vector, axis=0) #shape: 1x 300\n",
    "        vector = np.expand_dims(vector, axis=2) #shape: 1x 300 x 1\n",
    "        if len(word_filter_weights) == 0:\n",
    "            word_filter_weights = vector\n",
    "        else:\n",
    "            word_filter_weights = np.concatenate((word_filter_weights, vector), axis=2)\n",
    "    #shape is (1, 300, 533)\n",
    "    \n",
    "    bias_weights = np.zeros(len(senti2int))\n",
    "    cnn_wordfilter_weights = [word_filter_weights, bias_weights]\n",
    "    \n",
    "    return cnn_wordfilter_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 410)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_weights = Find_Filter_Weight(senti2int)\n",
    "CNN_weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "gru_dim = 50\n",
    "dropout_rate = 0.2\n",
    "atten_dim = 100\n",
    "# dense_dim = 30\n",
    "\n",
    "batch_size = 100\n",
    "epoch_num = 10\n",
    "\n",
    "categorical_label = True\n",
    "\n",
    "if categorical_label:\n",
    "    train_label_cat = np_utils.to_categorical(train_label)\n",
    "#     test_label_cat = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admusr/anaconda2/envs/python3_pengfei/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=50, kernel_size=3, strides=1, padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "/home/admusr/anaconda2/envs/python3_pengfei/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", weights=[array([[[..., trainable=False, filters=410, kernel_size=1, strides=1, padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=MAX_SENT_LENGTH, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "cnn_layer1 = Convolution1D(nb_filter=50,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1)\n",
    "\n",
    "cnn_layer2 = Convolution1D(nb_filter=CNN_weights[0].shape[2],\n",
    "                            filter_length=1,\n",
    "                            border_mode='same',\n",
    "                            activation='relu',\n",
    "                           weights = CNN_weights,\n",
    "                           trainable = False,\n",
    "                            subsample_length=1)\n",
    "\n",
    "rnn_layer = Bidirectional(GRU(gru_dim, activation='tanh', dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "# rnn_layer = GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)\n",
    "\n",
    "max_pooling_layer = GlobalMaxPooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 224, 300)     29148600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 224, 50)      45050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 224, 410)     123410      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM multiple             0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 460)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_1[1][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 460)          0           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 29,317,060\n",
      "Trainable params: 45,050\n",
      "Non-trainable params: 29,272,010\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "\n",
    "sent_cnn1 = cnn_layer1(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn1 = max_pooling_layer(sent_cnn1)  # output shape: (None, nb_filter)\n",
    "\n",
    "sent_cnn2 = cnn_layer2(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn2 = max_pooling_layer(sent_cnn2)  # output shape: (None, nb_filter)\n",
    "\n",
    "sent_cnn = concatenate([sent_cnn1, sent_cnn2])\n",
    "sent_cnn = Dropout(dropout_rate)(sent_cnn)\n",
    "\n",
    "sentEncoder = Model(sentence_input, sent_cnn)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 36, 224)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 36, 460)           29317060  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 36, 100)           153300    \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 29,480,762\n",
      "Trainable params: 208,752\n",
      "Non-trainable params: 29,272,010\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build document encoder model\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)   # out shape: (None, MAX_SENTS, nb_filter)\n",
    "\n",
    "rnn_out = rnn_layer(review_encoder) # (batch_size, timesteps, gru_dimx2)\n",
    "\n",
    "att_out = AttLayer(atten_dim)(rnn_out)\n",
    "# att_out = Dropout(dropout_rate)(att_out)\n",
    "\n",
    "# dense = Dense(dense_dim, activation='tanh')(att_out)\n",
    "# dense = Dropout(dropout_rate)(dense)\n",
    "\n",
    "if categorical_label:\n",
    "    preds = Dense(2, activation='softmax')(att_out) # categorical output\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "else:\n",
    "    preds = Dense(1, activation='sigmoid')(att_out)\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 60s 2ms/step - loss: 0.4932 - acc: 0.7472\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.8482\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.82247   0.88800   0.85398     12500\n",
      "          1    0.87830   0.80832   0.84186     12500\n",
      "\n",
      "avg / total    0.85038   0.84816   0.84792     25000\n",
      "\n",
      "Training for epoch 2/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 58s 2ms/step - loss: 0.3511 - acc: 0.8480\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 34s 1ms/step\n",
      "Accuracy: 0.8698\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.90897   0.82200   0.86330     12500\n",
      "          1    0.83754   0.91768   0.87578     12500\n",
      "\n",
      "avg / total    0.87326   0.86984   0.86954     25000\n",
      "\n",
      "Training for epoch 3/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.3231 - acc: 0.8625\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 34s 1ms/step\n",
      "Accuracy: 0.8806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.90343   0.85240   0.87717     12500\n",
      "          1    0.86029   0.90888   0.88392     12500\n",
      "\n",
      "avg / total    0.88186   0.88064   0.88054     25000\n",
      "\n",
      "Training for epoch 4/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.2940 - acc: 0.8788\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 34s 1ms/step\n",
      "Accuracy: 0.8773\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.83657   0.93776   0.88428     12500\n",
      "          1    0.92920   0.81680   0.86938     12500\n",
      "\n",
      "avg / total    0.88288   0.87728   0.87683     25000\n",
      "\n",
      "Training for epoch 5/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.2745 - acc: 0.8872\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 34s 1ms/step\n",
      "Accuracy: 0.8804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.83946   0.94080   0.88725     12500\n",
      "          1    0.93267   0.82008   0.87276     12500\n",
      "\n",
      "avg / total    0.88607   0.88044   0.88000     25000\n",
      "\n",
      "Training for epoch 6/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.2635 - acc: 0.8931\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.8942\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.87360   0.92168   0.89699     12500\n",
      "          1    0.91712   0.86664   0.89116     12500\n",
      "\n",
      "avg / total    0.89536   0.89416   0.89408     25000\n",
      "\n",
      "Training for epoch 7/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.2486 - acc: 0.8999\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.8898\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.85092   0.94520   0.89558     12500\n",
      "          1    0.93837   0.83440   0.88334     12500\n",
      "\n",
      "avg / total    0.89464   0.88980   0.88946     25000\n",
      "\n",
      "Training for epoch 8/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.2417 - acc: 0.9020\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.8990\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.92662   0.86672   0.89567     12500\n",
      "          1    0.87481   0.93136   0.90220     12500\n",
      "\n",
      "avg / total    0.90071   0.89904   0.89893     25000\n",
      "\n",
      "Training for epoch 9/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.2328 - acc: 0.9070\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.9030\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.89047   0.91896   0.90449     12500\n",
      "          1    0.91628   0.88696   0.90138     12500\n",
      "\n",
      "avg / total    0.90337   0.90296   0.90294     25000\n",
      "\n",
      "Training for epoch 10/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.2228 - acc: 0.9112\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.9002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.87837   0.92896   0.90295     12500\n",
      "          1    0.92462   0.87136   0.89720     12500\n",
      "\n",
      "avg / total    0.90149   0.90016   0.90008     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=5, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 61s 2ms/step - loss: 0.2195 - acc: 0.9112\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 36s 1ms/step\n",
      "Accuracy: 0.8977\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.86671   0.94000   0.90187     12500\n",
      "          1    0.93446   0.85544   0.89320     12500\n",
      "\n",
      "avg / total    0.90058   0.89772   0.89754     25000\n",
      "\n",
      "Training for epoch 2/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 61s 2ms/step - loss: 0.2069 - acc: 0.9189\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.9062\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.89126   0.92520   0.90791     12500\n",
      "          1    0.92224   0.88712   0.90434     12500\n",
      "\n",
      "avg / total    0.90675   0.90616   0.90613     25000\n",
      "\n",
      "Training for epoch 3/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 60s 2ms/step - loss: 0.2045 - acc: 0.9189\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 36s 1ms/step\n",
      "Accuracy: 0.9084\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.90690   0.91024   0.90857     12500\n",
      "          1    0.90991   0.90656   0.90823     12500\n",
      "\n",
      "avg / total    0.90841   0.90840   0.90840     25000\n",
      "\n",
      "Training for epoch 4/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 60s 2ms/step - loss: 0.1938 - acc: 0.9220\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.9024\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.93080   0.86944   0.89907     12500\n",
      "          1    0.87751   0.93536   0.90551     12500\n",
      "\n",
      "avg / total    0.90416   0.90240   0.90229     25000\n",
      "\n",
      "Training for epoch 5/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 60s 2ms/step - loss: 0.1901 - acc: 0.9238\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.8994\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.86783   0.94232   0.90354     12500\n",
      "          1    0.93690   0.85648   0.89489     12500\n",
      "\n",
      "avg / total    0.90236   0.89940   0.89921     25000\n",
      "\n",
      "Training for epoch 6/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.1844 - acc: 0.9279\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.9006\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.87047   0.94136   0.90453     12500\n",
      "          1    0.93616   0.85992   0.89642     12500\n",
      "\n",
      "avg / total    0.90331   0.90064   0.90047     25000\n",
      "\n",
      "Training for epoch 7/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.1804 - acc: 0.9305\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.9072\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.89268   0.92560   0.90884     12500\n",
      "          1    0.92275   0.88872   0.90542     12500\n",
      "\n",
      "avg / total    0.90771   0.90716   0.90713     25000\n",
      "\n",
      "Training for epoch 8/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.1715 - acc: 0.9326\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 34s 1ms/step\n",
      "Accuracy: 0.8868\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.83869   0.95792   0.89435     12500\n",
      "          1    0.95095   0.81576   0.87818     12500\n",
      "\n",
      "avg / total    0.89482   0.88684   0.88627     25000\n",
      "\n",
      "Training for epoch 9/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 57s 2ms/step - loss: 0.1599 - acc: 0.9385\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.9061\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.88939   0.92760   0.90809     12500\n",
      "          1    0.92435   0.88464   0.90406     12500\n",
      "\n",
      "avg / total    0.90687   0.90612   0.90608     25000\n",
      "\n",
      "Training for epoch 10/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.1599 - acc: 0.9394\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Accuracy: 0.9035\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.92694   0.87600   0.90075     12500\n",
      "          1    0.88246   0.93096   0.90606     12500\n",
      "\n",
      "avg / total    0.90470   0.90348   0.90341     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=5, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
