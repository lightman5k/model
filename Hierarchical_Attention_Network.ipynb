{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admusr/anaconda2/envs/python3_pengfei/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\"         # 3 is can change to 0-3\n",
    "gpu_num = 3\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda, Masking\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util.util_functions import getWordIdx\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train_copus_padded data from .pickle file\n",
    "file = open('pickle_data/train_copus_pad.pickle','rb')\n",
    "train_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_copus_pad.pickle','rb')\n",
    "test_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','rb')\n",
    "vocab_to_int_train = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/embedding_matrix','rb')\n",
    "embedding_matrix = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/train_label.pickle','rb')\n",
    "train_label = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_label.pickle','rb')\n",
    "test_label = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test data shape: (25000, 30, 235) (25000, 30, 235)\n",
      "embedding_matrix shape: (106180, 300)\n",
      "vocabulary size: 106180\n",
      "max sent number in a review: 30 \n",
      "max words in a sentence: 235\n"
     ]
    }
   ],
   "source": [
    "print('train test data shape:',train_copus_padded.shape, test_copus_padded.shape)\n",
    "print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int_train)\n",
    "print('vocabulary size:', vocab_size)\n",
    "# the maximal length of every sentence\n",
    "MAX_SENTS = train_copus_padded.shape[1]\n",
    "MAX_SENT_LENGTH = train_copus_padded.shape[2]\n",
    "print('max sent number in a review:', MAX_SENTS, '\\nmax words in a sentence:', MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention layer\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # current shape: [batch_size, set_len]\n",
    "        mask = None\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "gru_dim = 100\n",
    "dropout_rate = 0.3\n",
    "atten_dim = 100\n",
    "dense_dim = 50\n",
    "\n",
    "batch_size = 100\n",
    "epoch_num = 10\n",
    "\n",
    "categorical_label = True\n",
    "\n",
    "if categorical_label:\n",
    "    train_label_cat = np_utils.to_categorical(train_label)\n",
    "#     test_label_cat = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=MAX_SENT_LENGTH, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "rnn_layer1 = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "rnn_layer2 = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "# rnn_layer = GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)\n",
    "\n",
    "\n",
    "# cnn_layer = Convolution1D(nb_filter=50,filter_length=3,border_mode='same',activation='tanh',subsample_length=1)\n",
    "# max_pooling_layer = GlobalMaxPooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 235)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 235, 300)          31854000  \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 235, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 235, 200)          240600    \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 200)               20200     \n",
      "=================================================================\n",
      "Total params: 32,114,800\n",
      "Trainable params: 260,800\n",
      "Non-trainable params: 31,854,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "# mask out padding tokens\n",
    "sent_embedding = Masking(mask_value=0., input_shape=(MAX_SENT_LENGTH, embedding_matrix.shape[1]))(sent_embedding)\n",
    "\n",
    "sent_lstm = rnn_layer1(sent_embedding)\n",
    "sent_att = AttLayer(atten_dim)(sent_lstm)\n",
    "\n",
    "sentEncoder = Model(sentence_input, sent_att)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 30, 235)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 200)           32114800  \n",
      "_________________________________________________________________\n",
      "masking_2 (Masking)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 30, 200)           180600    \n",
      "_________________________________________________________________\n",
      "att_layer_2 (AttLayer)       (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 32,325,752\n",
      "Trainable params: 471,752\n",
      "Non-trainable params: 31,854,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 30, 235)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 30, 235)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 30, 235)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 30, 235)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 2)            32325752    lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Concatenate)           (None, 2)            0           model_2[1][0]                    \n",
      "                                                                 model_2[2][0]                    \n",
      "                                                                 model_2[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 32,325,752\n",
      "Trainable params: 471,752\n",
      "Non-trainable params: 31,854,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build document encoder model\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)   # out shape: (None, MAX_SENTS, gru_dim*2)\n",
    "# mask out padding sentences\n",
    "review_encoder = Masking(mask_value=0., input_shape=(MAX_SENTS, gru_dim*2))(review_encoder)\n",
    "\n",
    "rnn_out = rnn_layer2(review_encoder) # (batch_size, timesteps, gru_dimx2)\n",
    "\n",
    "att_out = AttLayer(atten_dim)(rnn_out)\n",
    "# att_out = Dropout(dropout_rate)(att_out)\n",
    "\n",
    "dense = Dense(dense_dim, activation='tanh')(att_out)\n",
    "dense = Dropout(dropout_rate)(dense)\n",
    "\n",
    "if categorical_label:\n",
    "    preds = Dense(2, activation='softmax')(dense) # categorical output\n",
    "    model = Model(review_input, preds)\n",
    "    print(model.summary())\n",
    "    # Replicates `model` on multiple GPUs.\n",
    "    # This assumes that your machine has 'gpus' available GPUs.\n",
    "    model = multi_gpu_model(model, gpus=gpu_num)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "    \n",
    "else:\n",
    "    preds = Dense(1, activation='sigmoid')(dense)\n",
    "    model = Model(review_input, preds)\n",
    "    print(model.summary())\n",
    "    # Replicates `model` on multiple GPUs.\n",
    "    # This assumes that your machine has 'gpus' available GPUs.\n",
    "    model = multi_gpu_model(model, gpus=gpu_num)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 179s 7ms/step - loss: 0.3868 - acc: 0.8185\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 75s 3ms/step\n",
      "Accuracy: 0.8854\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.88267   0.88888   0.88576     12500\n",
      "          1    0.88809   0.88184   0.88496     12500\n",
      "\n",
      "avg / total    0.88538   0.88536   0.88536     25000\n",
      "\n",
      "Training for epoch 2/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 169s 7ms/step - loss: 0.2909 - acc: 0.8805\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 73s 3ms/step\n",
      "Accuracy: 0.8954\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.88896   0.90368   0.89626     12500\n",
      "          1    0.90206   0.88712   0.89453     12500\n",
      "\n",
      "avg / total    0.89551   0.89540   0.89539     25000\n",
      "\n",
      "Training for epoch 3/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 168s 7ms/step - loss: 0.2732 - acc: 0.8888\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 72s 3ms/step\n",
      "Accuracy: 0.8995\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.89409   0.90632   0.90016     12500\n",
      "          1    0.90502   0.89264   0.89879     12500\n",
      "\n",
      "avg / total    0.89955   0.89948   0.89948     25000\n",
      "\n",
      "Training for epoch 4/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 172s 7ms/step - loss: 0.2626 - acc: 0.8946\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 76s 3ms/step\n",
      "Accuracy: 0.8944\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.93307   0.84984   0.88951     12500\n",
      "          1    0.86214   0.93904   0.89895     12500\n",
      "\n",
      "avg / total    0.89760   0.89444   0.89423     25000\n",
      "\n",
      "Training for epoch 5/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 172s 7ms/step - loss: 0.2531 - acc: 0.8958\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 73s 3ms/step\n",
      "Accuracy: 0.9072\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.91275   0.90056   0.90662     12500\n",
      "          1    0.90187   0.91392   0.90786     12500\n",
      "\n",
      "avg / total    0.90731   0.90724   0.90724     25000\n",
      "\n",
      "Training for epoch 6/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 170s 7ms/step - loss: 0.2433 - acc: 0.9027\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 76s 3ms/step\n",
      "Accuracy: 0.9090\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.90400   0.91528   0.90960     12500\n",
      "          1    0.91421   0.90280   0.90847     12500\n",
      "\n",
      "avg / total    0.90910   0.90904   0.90904     25000\n",
      "\n",
      "Training for epoch 7/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 171s 7ms/step - loss: 0.2360 - acc: 0.9045\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 75s 3ms/step\n",
      "Accuracy: 0.9037\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.93485   0.86784   0.90010     12500\n",
      "          1    0.87668   0.93952   0.90701     12500\n",
      "\n",
      "avg / total    0.90576   0.90368   0.90356     25000\n",
      "\n",
      "Training for epoch 8/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 169s 7ms/step - loss: 0.2265 - acc: 0.9085\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 74s 3ms/step\n",
      "Accuracy: 0.9078\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.88867   0.93232   0.90997     12500\n",
      "          1    0.92882   0.88320   0.90544     12500\n",
      "\n",
      "avg / total    0.90875   0.90776   0.90770     25000\n",
      "\n",
      "Training for epoch 9/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 171s 7ms/step - loss: 0.2183 - acc: 0.9142\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 74s 3ms/step\n",
      "Accuracy: 0.9003\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.94807   0.84704   0.89471     12500\n",
      "          1    0.86177   0.95360   0.90536     12500\n",
      "\n",
      "avg / total    0.90492   0.90032   0.90004     25000\n",
      "\n",
      "Training for epoch 10/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 171s 7ms/step - loss: 0.2142 - acc: 0.9144\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 76s 3ms/step\n",
      "Accuracy: 0.9135\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.91432   0.91256   0.91344     12500\n",
      "          1    0.91273   0.91448   0.91360     12500\n",
      "\n",
      "avg / total    0.91352   0.91352   0.91352     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=5, labels=[0, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
