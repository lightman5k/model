{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admusr/anaconda2/envs/python3_pengfei/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"         # 3 is can change to 0-3\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util.util_functions import getWordIdx\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train_copus_padded data from .pickle file\n",
    "file = open('pickle_data/train_copus_pad.pickle','rb')\n",
    "train_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_copus_pad.pickle','rb')\n",
    "test_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','rb')\n",
    "vocab_to_int_train = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/embedding_matrix','rb')\n",
    "embedding_matrix = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/train_label.pickle','rb')\n",
    "train_label = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_label.pickle','rb')\n",
    "test_label = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test data shape: (25000, 36, 224) (25000, 36, 224)\n",
      "embedding_matrix shape: (97162, 300)\n",
      "vocabulary size: 97162\n",
      "max sent number in a review: 36 \n",
      "max words in a sentence: 224\n"
     ]
    }
   ],
   "source": [
    "print('train test data shape:',train_copus_padded.shape, test_copus_padded.shape)\n",
    "print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int_train)\n",
    "print('vocabulary size:', vocab_size)\n",
    "# the maximal length of every sentence\n",
    "MAX_SENTS = train_copus_padded.shape[1]\n",
    "MAX_SENT_LENGTH = train_copus_padded.shape[2]\n",
    "print('max sent number in a review:', MAX_SENTS, '\\nmax words in a sentence:', MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "gru_dim = 50\n",
    "dropout_rate = 0.3\n",
    "atten_dim = 50\n",
    "dense_dim = 50\n",
    "\n",
    "batch_size = 100\n",
    "epoch_num = 10\n",
    "\n",
    "categorical_label = True\n",
    "\n",
    "if categorical_label:\n",
    "    train_label_cat = np_utils.to_categorical(train_label)\n",
    "#     test_label_cat = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admusr/anaconda2/envs/python3_pengfei/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=100, kernel_size=3, strides=1, padding=\"same\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=MAX_SENT_LENGTH, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "cnn_layer1 = Convolution1D(nb_filter=100,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "rnn_layer = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "# rnn_layer = GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=False)\n",
    "\n",
    "max_pooling_layer = GlobalMaxPooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 224, 300)          29148600  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 224, 100)          105300    \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 100)               5100      \n",
      "=================================================================\n",
      "Total params: 29,259,000\n",
      "Trainable params: 110,400\n",
      "Non-trainable params: 29,148,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "\n",
    "sent_rnn = rnn_layer(sent_embedding) # output shape: (None, gru_dim*2)\n",
    "\n",
    "att_out = AttLayer(atten_dim)(sent_rnn)\n",
    "# att_out = Dropout(dropout_rate)(att_out)\n",
    "\n",
    "sentEncoder = Model(sentence_input, att_out)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 36, 224)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 36, 100)           29259000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 36, 100)           30100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 29,289,302\n",
      "Trainable params: 140,702\n",
      "Non-trainable params: 29,148,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build document encoder model\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)   # out shape: (None, MAX_SENTS, gru_dim*2)\n",
    "\n",
    "cnn_out = cnn_layer1(review_encoder) # (batch_size, timesteps, nb_filter)\n",
    "cnn_out = max_pooling_layer(cnn_out)  # output shape: (batch_size, nb_filter)\n",
    "\n",
    "\n",
    "# dense = Dense(dense_dim, activation='tanh')(cnn_out)\n",
    "# dense = Dropout(dropout_rate)(dense)\n",
    "\n",
    "if categorical_label:\n",
    "    preds = Dense(2, activation='softmax')(cnn_out) # categorical output\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "else:\n",
    "    preds = Dense(1, activation='sigmoid')(cnn_out)\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 346s 14ms/step - loss: 0.4359 - acc: 0.7826\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 83s 3ms/step\n",
      "Accuracy: 0.8693\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8457    0.9035    0.8736     12500\n",
      "          1     0.8964    0.8351    0.8647     12500\n",
      "\n",
      "avg / total     0.8711    0.8693    0.8692     25000\n",
      "\n",
      "Training for epoch 2/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 341s 14ms/step - loss: 0.2886 - acc: 0.8788\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 83s 3ms/step\n",
      "Accuracy: 0.8820\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9141    0.8434    0.8773     12500\n",
      "          1     0.8546    0.9207    0.8864     12500\n",
      "\n",
      "avg / total     0.8843    0.8820    0.8819     25000\n",
      "\n",
      "Training for epoch 3/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 342s 14ms/step - loss: 0.2627 - acc: 0.8909\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 84s 3ms/step\n",
      "Accuracy: 0.8805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8394    0.9411    0.8873     12500\n",
      "          1     0.9330    0.8199    0.8728     12500\n",
      "\n",
      "avg / total     0.8862    0.8805    0.8801     25000\n",
      "\n",
      "Training for epoch 4/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 343s 14ms/step - loss: 0.2406 - acc: 0.9016\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 84s 3ms/step\n",
      "Accuracy: 0.8959\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8806    0.9159    0.8979     12500\n",
      "          1     0.9124    0.8758    0.8938     12500\n",
      "\n",
      "avg / total     0.8965    0.8959    0.8958     25000\n",
      "\n",
      "Training for epoch 5/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 342s 14ms/step - loss: 0.2192 - acc: 0.9133\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 84s 3ms/step\n",
      "Accuracy: 0.9010\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8975    0.9054    0.9014     12500\n",
      "          1     0.9045    0.8966    0.9005     12500\n",
      "\n",
      "avg / total     0.9010    0.9010    0.9010     25000\n",
      "\n",
      "Training for epoch 6/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 341s 14ms/step - loss: 0.2055 - acc: 0.9180\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 83s 3ms/step\n",
      "Accuracy: 0.8912\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8533    0.9448    0.8967     12500\n",
      "          1     0.9382    0.8376    0.8850     12500\n",
      "\n",
      "avg / total     0.8957    0.8912    0.8909     25000\n",
      "\n",
      "Training for epoch 7/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 339s 14ms/step - loss: 0.1870 - acc: 0.9268\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 82s 3ms/step\n",
      "Accuracy: 0.9028\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8953    0.9122    0.9037     12500\n",
      "          1     0.9105    0.8934    0.9018     12500\n",
      "\n",
      "avg / total     0.9029    0.9028    0.9028     25000\n",
      "\n",
      "Training for epoch 8/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 345s 14ms/step - loss: 0.1732 - acc: 0.9332\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 82s 3ms/step\n",
      "Accuracy: 0.9028\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9036    0.9018    0.9027     12500\n",
      "          1     0.9020    0.9038    0.9029     12500\n",
      "\n",
      "avg / total     0.9028    0.9028    0.9028     25000\n",
      "\n",
      "Training for epoch 9/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 349s 14ms/step - loss: 0.1529 - acc: 0.9409\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 82s 3ms/step\n",
      "Accuracy: 0.9017\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9151    0.8856    0.9001     12500\n",
      "          1     0.8892    0.9178    0.9033     12500\n",
      "\n",
      "avg / total     0.9021    0.9017    0.9017     25000\n",
      "\n",
      "Training for epoch 10/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 348s 14ms/step - loss: 0.1353 - acc: 0.9492\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 83s 3ms/step\n",
      "Accuracy: 0.9008\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9210    0.8769    0.8984     12500\n",
      "          1     0.8825    0.9248    0.9032     12500\n",
      "\n",
      "avg / total     0.9018    0.9008    0.9008     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 347s 14ms/step - loss: 0.1135 - acc: 0.9584\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 82s 3ms/step\n",
      "Accuracy: 0.8996\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8868    0.9161    0.9012     12500\n",
      "          1     0.9132    0.8831    0.8979     12500\n",
      "\n",
      "avg / total     0.9000    0.8996    0.8996     25000\n",
      "\n",
      "Training for epoch 2/10\n",
      "Epoch 1/1\n",
      " 3900/25000 [===>..........................] - ETA: 4:56 - loss: 0.0826 - acc: 0.9741"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
